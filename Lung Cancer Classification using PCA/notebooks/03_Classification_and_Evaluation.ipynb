{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Classification and Evaluation on Reduced Lung Data\n",
    "\n",
    "## Overview\n",
    "This notebook implements multiple classification algorithms and evaluates their performance on dimensionally reduced lung microRNA data using various preprocessing techniques.\n",
    "\n",
    "## Table of Contents\n",
    "1. Data Preparation and Standardization\n",
    "2. Minimum Distance Classifier\n",
    "3. Bayes Classifier\n",
    "4. Naive Bayes Classifier\n",
    "5. K-Nearest Neighbors (KNN)\n",
    "6. Linear Discriminant Analysis (LDA)\n",
    "7. Kernel Discriminant Analysis (KDA)\n",
    "8. Performance Evaluation\n",
    "\n",
    "## Dataset Information\n",
    "- **Dataset**: Lung.csv (dimensionally reduced)\n",
    "- **Train/Test Split**: 872/219 samples\n",
    "- **Classification Task**: Lung cancer classification\n",
    "- **Evaluation Metrics**: Accuracy, Precision, Recall, F1-Score\n",
    "\n",
    "## Author\n",
    "- Raja Ram Bitra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers:\n",
    "- Imagine you have all your lung cancer patient data, and you want to build a tool that can look at a new patient's data and predict if they have cancer or not. That tool is called a classifier.\n",
    "- **Definition:** A classifier is like a decision-maker. It learns from your existing, labeled data (where you already know who has cancer and who doesn't) to figure out patterns. Once it has learned, it can then take new, unlabelled data and make an educated guess about which group it belongs to (e.g., \"cancer\" or \"no cancer\").\n",
    "\n",
    "**Types**\n",
    "\n",
    "**1. Minimum Distance Classifier (MDC)**\n",
    "- **What it does:** This classifier is like finding the \"average\" patient for each group (cancerous and non-cancerous). When a new patient comes in, it simply checks which group's \"average\" patient they are closest to, and assigns them to that group. It's very simple and just measures direct distance.\n",
    "- **When it's useful:** Use this if you believe that patients with cancer are generally quite different from non-cancerous patients, and these differences can be captured by looking at their average feature values. For example, if all cancer patients consistently have a very high average of one specific biomarker.\n",
    "\n",
    "**2. Bayes Classifier (Optimal Bayes Classifier)**\n",
    "- **What it does:** This is the \"smartest\" theoretical decision-maker. It tries to calculate the exact probability of a patient having cancer given their specific features. It then assigns the patient to the group (cancer or no cancer) that has the highest probability. It's \"optimal\" because, in theory, it makes the best possible decision if you know all the true probabilities.\n",
    "- **When it's useful:** While often theoretical (because getting true probabilities is hard), it's a benchmark. It tells you what's the absolute best you could do. If your data perfectly captures the real-world probabilities, this would be perfect. It's the goal other classifiers try to approximate.\n",
    "\n",
    "**3. Naive Bayes Classifier**\n",
    "- **What it does:** This is a simplified version of the Bayes Classifier. It makes a \"naive\" (simplifying) assumption: it assumes that all your patient features (like gene expression, tumor size, etc.) are independent of each other given the patient's cancer status. So, knowing a patient has a large tumor doesn't change the probability of them having a high biomarker, if you already know they have cancer. Despite this simplification, it often works surprisingly well.\n",
    "- **When it's useful:** It's a good choice if your features are somewhat independent, or if you have a lot of data. It's very fast to train and can perform well even with limited computational resources. For example, if different biomarkers each provide unique, non-overlapping clues about cancer.\n",
    "\n",
    "**4. K-Nearest Neighbors (KNN)**\n",
    "- **What it does:** KNN is like consulting your \"neighborhood\" of past patients. When a new patient comes in, it looks at the 'K' (a number you choose, like 3 or 5) most similar past patients. If most of those 'K' neighbors had cancer, then it predicts the new patient also has cancer. It classifies based on the majority vote of its closest friends.\n",
    "- **When it's useful:** This is good if you believe that patients with similar characteristics should have the same diagnosis. It's simple to understand and works well when your data has clear, well-defined clusters for each class.\n",
    "\n",
    "**5. Linear Discriminant Analysis (LDA)**\n",
    "- **What it does:** LDA is a bit like PCA, but specifically for classification. Instead of just finding components that capture data variance, it finds new \"directions\" in your data that best separate your groups (cancerous vs. non-cancerous). It tries to draw straight lines or planes that optimally divide the classes.\n",
    "- **When it's useful:** Use LDA if you think your different patient groups (cancer vs. non-cancer) can be effectively separated by a straight line or flat plane based on their features. It's good when your data classes are somewhat distinct and normally distributed.\n",
    "\n",
    "**6. Kernel Discriminant Analysis (KDA)**\n",
    "- **What it does:** KDA is the \"smarter\" version of LDA, just like KPCA is to PCA. If the separation between your cancer and non-cancer groups isn't a straight line, but a complex curve, KDA can handle it. It uses the \"kernel trick\" (like in KPCA) to project your data into a higher-dimensional space where those complex, curved separations become straight lines, then applies LDA there.\n",
    "- **When it's useful:** This is powerful when the boundary between cancerous and non-cancerous patients is non-linear and complex. If your data points for cancer and non-cancer are intertwined in a way that a straight line can't separate them, KDA can find that curvy boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.metrics.pairwise import rbf_kernel, polynomial_kernel, linear_kernel\n",
    "from scipy.linalg import solve, pinv\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3a. Data Preparation and Standardization\n",
    "\n",
    "- Loading the lung cancer dataset, applying dimensionality reduction via PCA, and standardizing features for optimal classifier performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training shape: (872, 1881)\n",
      "PCA training shape: (872, 8)\n",
      "Components retained: 8\n",
      "Data standardized and ready for classification.\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv('../Lung.csv')\n",
    "data = df.iloc[:,:-1].to_numpy()\n",
    "label = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Whole data\n",
    "X_train_whole, X_test_whole = X_train, X_test\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(f\"Original training shape: {X_train.shape}\")\n",
    "print(f\"PCA training shape: {X_train_pca.shape}\")\n",
    "print(f\"Components retained: {pca.n_components_}\")\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_pca)\n",
    "X_test_scaled = scaler.transform(X_test_pca)\n",
    "\n",
    "print(\"Data standardized and ready for classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Classifier Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Minimum Distance Classifier\n",
    "\n",
    "- Implementation of a simple distance-based classifier that assigns samples to the closest class centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum Distance Classifier (build from scratch)\n",
    "\n",
    "def min_distance_classifier(X_train, y_train, X_test):\n",
    "    classes = np.unique(y_train)\n",
    "    class_means = {}\n",
    "    for c in classes:\n",
    "        class_means[c] = np.mean(X_train[y_train == c], axis=0)\n",
    "    \n",
    "    predictions = []\n",
    "    for x in X_test:\n",
    "        dists = [np.linalg.norm(x-class_means[c]) for c in classes]\n",
    "        predictions.append(classes[np.argmin(dists)])\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 3c. Bayes Classifier (From Scratch)\n",
    "- Implementation of Bayes classifier using Gaussian assumption for likelihood calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes Classifier (build from scratch)\n",
    "\n",
    "class BayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.priors = None\n",
    "        self.mean = None\n",
    "        self.variance = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = len(self.classes)\n",
    "        # Initialize arrays to store class-wise statistics\n",
    "        self.mean = np.zeros((n_classes, n_features))\n",
    "        self.variance = np.zeros((n_classes, n_features))\n",
    "        self.priors = np.zeros(n_classes)\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            X_c = X[y == cls]\n",
    "            self.mean[idx, :] = X_c.mean(axis=0)\n",
    "            self.variance[idx, :] = X_c.var(axis=0)\n",
    "            self.priors[idx] = X_c.shape[0] / float(X.shape[0])\n",
    "    \n",
    "    def _calculate_likelihood(self, mean, var, x):\n",
    "        eps = 1e-6  # Add small epsilon to variance to avoid division by zero\n",
    "        coeff = 1 / np.sqrt(2 * np.pi * var + eps)\n",
    "        exponent = -((x - mean) ** 2) / (2 * (var + eps))\n",
    "        return coeff * np.exp(exponent)\n",
    "    \n",
    "    def _calculate_posterior(self, X):\n",
    "        posteriors = []\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            prior = np.log(self.priors[idx])\n",
    "            likelihood = np.sum(np.log(self._calculate_likelihood(self.mean[idx, :], self.variance[idx, :], X)), axis=1)\n",
    "            posteriors.append(prior + likelihood)\n",
    "        return np.array(posteriors).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        posteriors = self._calculate_posterior(X)\n",
    "        return self.classes[np.argmax(posteriors, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 3d. Naive Bayes Classifier\n",
    "- Creating instances of standard classifiers from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes classifier initialized\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier\n",
    "naive_bayes = GaussianNB()\n",
    "print(\"Naive Bayes classifier initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. K-Nearest Neighbors Classifier\n",
    "- Creating instances of standard classifiers from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN classifier initialized with k=10\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "print(\"KNN classifier initialized with k=10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3f. Linear Discriminant Analysis\n",
    "- Creating instances of standard classifiers from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA classifier initialized\n"
     ]
    }
   ],
   "source": [
    "# Linear Discriminant Analysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "print(\"LDA classifier initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 3g. Kernel Discriminant Analysis (KDA) - From Scratch\n",
    "- Implementation of Kernel Discriminant Analysis with different kernel functions (RBF, Polynomial, Linear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build KDA classifier \n",
    "class KernelDiscriminantAnalysis:\n",
    "    def __init__(self, kernel='linear', degree=3, coef0=1, gamma=None, reg=1e-3):\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.coef0 = coef0\n",
    "        self.gamma = gamma\n",
    "        self.reg = reg\n",
    "        self.eigenvectors = None\n",
    "        self.class_means = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.label_dict = {}\n",
    "\n",
    "    def compute_kernel(self, X, Y=None):\n",
    "        if self.kernel == 'rbf':\n",
    "            return rbf_kernel(X, Y, gamma=self.gamma)\n",
    "        elif self.kernel == 'poly':\n",
    "            return polynomial_kernel(X, Y, degree=self.degree, coef0=self.coef0)\n",
    "        elif self.kernel == 'linear':\n",
    "            return linear_kernel(X, Y)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported kernel. Choose from 'rbf', 'poly', or 'linear'.\")\n",
    "\n",
    "    def encode_labels(self, y):\n",
    "        unique_classes = np.unique(y)\n",
    "        self.label_dict = {label: idx for idx, label in enumerate(unique_classes)}\n",
    "        return np.array([self.label_dict[label] for label in y])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = self.encode_labels(y)\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        K = self.compute_kernel(X)\n",
    "\n",
    "        classes = np.unique(self.y_train)\n",
    "        N_c, K_c, mean_c = {}, {}, {}\n",
    "\n",
    "        # Compute class-wise kernel matrices\n",
    "        for c in classes:\n",
    "            idx = np.where(self.y_train == c)[0]\n",
    "            K_c[c] = K[:, idx]\n",
    "            N_c[c] = len(idx)\n",
    "            mean_c[c] = np.mean(K_c[c], axis=1, keepdims=True)\n",
    "\n",
    "        # Compute between-class scatter matrix M\n",
    "        mean_total = np.mean(K, axis=1, keepdims=True)\n",
    "        M = np.zeros((n_samples, n_samples))\n",
    "        for c in classes:\n",
    "            diff = mean_c[c] - mean_total\n",
    "            M += N_c[c] * (diff @ diff.T)\n",
    "\n",
    "        # Compute within-class scatter matrix N\n",
    "        N = np.zeros((n_samples, n_samples))\n",
    "        for c in classes:\n",
    "            N += K_c[c] @ (np.eye(N_c[c]) - (1 / N_c[c]) * np.ones((N_c[c], N_c[c]))) @ K_c[c].T\n",
    "\n",
    "        # Regularize N to ensure positive definiteness\n",
    "        N += np.eye(N.shape[0]) * self.reg\n",
    "\n",
    "        # Solve the generalized eigenvalue problem using np.linalg.solve()\n",
    "        try:\n",
    "            eigvals, eigvecs = np.linalg.eig(solve(N, M))  # Solve for eigenvectors\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Warning: N is singular, using pseudo-inverse instead.\")\n",
    "            eigvals, eigvecs = np.linalg.eig(pinv(N) @ M)\n",
    "\n",
    "        # Select top discriminant directions\n",
    "        idx = np.argsort(-eigvals)\n",
    "        self.eigenvectors = eigvecs[:, idx[:len(classes) - 1]]\n",
    "\n",
    "        # Normalize eigenvectors\n",
    "        self.eigenvectors /= np.linalg.norm(self.eigenvectors, axis=0)\n",
    "\n",
    "        # Compute class means in the transformed space\n",
    "        self.class_means = {}\n",
    "        transformed_X = self.transform(X)\n",
    "        for c in classes:\n",
    "            self.class_means[c] = np.mean(transformed_X[self.y_train == c], axis=0)\n",
    "\n",
    "    def transform(self, X):\n",
    "        K_test = self.compute_kernel(X, self.X_train)\n",
    "        return K_test @ self.eigenvectors\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_proj = self.transform(X)\n",
    "        predictions = []\n",
    "        for x in X_proj:\n",
    "            # Assign to nearest class mean\n",
    "            distances = {c: np.linalg.norm(x - self.class_means[c]) for c in self.class_means}\n",
    "            predictions.append(min(distances, key=distances.get))\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Data Variants Preparation\n",
    "- Creating different data transformations to evaluate classifier performance across various feature spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCA Results\n",
      "PCA n_components retained: 8\n",
      "X_train_pca shape: (872, 8)\n",
      "X_test_pca shape:  (219, 8)\n",
      "\n",
      "RBF Kernel PCA Results\n",
      "X_train_kpca_rbf shape: (872, 8)\n",
      "X_test_kpca_rbf shape:  (219, 8)\n",
      "\n",
      "Polynomial Kernel PCA Results\n",
      "X_train_kpca_poly shape: (872, 8)\n",
      "X_test_kpca_poly shape:  (219, 8)\n",
      "\n",
      "Linear Kernel PCA Results\n",
      "X_train_kpca_lin shape: (872, 8)\n",
      "X_test_kpca_lin shape:  (219, 8)\n"
     ]
    }
   ],
   "source": [
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nPCA Results\")\n",
    "print(\"PCA n_components retained:\", pca.n_components_)\n",
    "print(\"X_train_pca shape:\", X_train_pca.shape)\n",
    "print(\"X_test_pca shape: \", X_test_pca.shape)\n",
    "\n",
    "# Using the same number of components in KPCA that are attained from above PCA \n",
    "# RBF Kernel PCA\n",
    "kpca_rbf = KernelPCA(n_components=pca.n_components_, kernel='rbf', gamma=0.1)\n",
    "kpca_rbf.fit(X_train_scaled)\n",
    "\n",
    "X_train_kpca_rbf = kpca_rbf.transform(X_train_scaled)\n",
    "X_test_kpca_rbf  = kpca_rbf.transform(X_test_scaled)\n",
    "\n",
    "print(\"\\nRBF Kernel PCA Results\")\n",
    "print(\"X_train_kpca_rbf shape:\", X_train_kpca_rbf.shape)\n",
    "print(\"X_test_kpca_rbf shape: \", X_test_kpca_rbf.shape)\n",
    "\n",
    "# Polynomial Kernel PCA\n",
    "kpca_poly = KernelPCA(n_components=pca.n_components_, kernel='poly', degree=3, coef0=1, gamma=0.1)\n",
    "kpca_poly.fit(X_train_scaled)\n",
    "\n",
    "X_train_kpca_poly = kpca_poly.transform(X_train_scaled)\n",
    "X_test_kpca_poly  = kpca_poly.transform(X_test_scaled)\n",
    "\n",
    "print(\"\\nPolynomial Kernel PCA Results\")\n",
    "print(\"X_train_kpca_poly shape:\", X_train_kpca_poly.shape)\n",
    "print(\"X_test_kpca_poly shape: \", X_test_kpca_poly.shape)\n",
    "\n",
    "# Linear Kernel PCA\n",
    "kpca_lin = KernelPCA(n_components=pca.n_components_, kernel='linear')\n",
    "kpca_lin.fit(X_train_scaled)\n",
    "\n",
    "X_train_kpca_lin = kpca_lin.transform(X_train_scaled)\n",
    "X_test_kpca_lin  = kpca_lin.transform(X_test_scaled)\n",
    "\n",
    "print(\"\\nLinear Kernel PCA Results\")\n",
    "print(\"X_train_kpca_lin shape:\", X_train_kpca_lin.shape)\n",
    "print(\"X_test_kpca_lin shape: \", X_test_kpca_lin.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 feature indices by variance: [ 426 1787 1344 1343  815  556 1573 1798   97  741]\n",
      "Shape of X_train_top10: (872, 10)\n",
      "Shape of X_test_top10: (219, 10)\n",
      "Accuracy using top 10 variance features: 0.5114155251141552\n"
     ]
    }
   ],
   "source": [
    "def get_top10_features(X_train, X_test, y_train, y_test, C=0.1):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    cov_matrix = np.cov(X_train_scaled, rowvar=False) # Covariance matrix\n",
    "\n",
    "    variances = np.diag(cov_matrix)\n",
    "    top10_indices = np.argsort(variances)[::-1][:10]\n",
    "    print(\"\\nTop 10 feature indices by variance:\", top10_indices)\n",
    "\n",
    "    X_train_top10 = X_train_scaled[:, top10_indices] \n",
    "    X_test_top10 = X_test_scaled[:, top10_indices]\n",
    "\n",
    "    print(\"Shape of X_train_top10:\", X_train_top10.shape)\n",
    "    print(\"Shape of X_test_top10:\", X_test_top10.shape)\n",
    "\n",
    "    # Train an SVM classifier\n",
    "    classifier = SVC(kernel='linear', C=C)\n",
    "    classifier.fit(X_train_top10, y_train)\n",
    "\n",
    "    # Predict and evaluating accuracy\n",
    "    y_pred = classifier.predict(X_test_top10)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy using top 10 variance features:\", accuracy)\n",
    "\n",
    "    return accuracy, top10_indices, X_train_top10, X_test_top10\n",
    "\n",
    "accuracy, top10_indices, X_train_top10, X_test_top10 = get_top10_features(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL DATA VARIANTS SUMMARY\n",
      "============================================================\n",
      "- Whole standardized data: (872, 1881)\n",
      "- PCA: (872, 8)\n",
      "- KPCA RBF: (872, 8)\n",
      "- KPCA Polynomial: (872, 8)\n",
      "- KPCA Linear: (872, 8)\n",
      "- Top 10 features): (872, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL DATA VARIANTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"- Whole standardized data: {X_train_whole.shape}\")\n",
    "print(f\"- PCA: {X_train_pca.shape}\")\n",
    "print(f\"- KPCA RBF: {X_train_kpca_rbf.shape}\")\n",
    "print(f\"- KPCA Polynomial: {X_train_kpca_poly.shape}\")\n",
    "print(f\"- KPCA Linear: {X_train_kpca_lin.shape}\")\n",
    "print(f\"- Top 10 features): {X_train_top10.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dictionary of data variants\n",
    "data_variants = {\n",
    "    'whole': (X_train_whole, X_test_whole),\n",
    "    'pca': (X_train_pca, X_test_pca),\n",
    "    'kpca_rbf': (X_train_kpca_rbf, X_test_kpca_rbf),\n",
    "    'kpca_poly': (X_train_kpca_poly, X_test_kpca_poly),\n",
    "    'kpca_lin': (X_train_kpca_lin, X_test_kpca_lin),\n",
    "    'top10': (X_train_top10, X_test_top10)\n",
    "}\n",
    "   \n",
    "# List of classifiers for future use\n",
    "classifiers = [\n",
    "    'min_dist',\n",
    "    'BayesClassifier',\n",
    "    'naive_bayes',\n",
    "    'knn',\n",
    "    'lda',\n",
    "    'kda_rbf',\n",
    "    'kda_poly',\n",
    "    'kda_linear'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Performance Evaluation\n",
    "- Setting up the evaluation framework with data variants and classifier execution functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(clf_name, X_train, y_train, X_test, y_test=None):\n",
    "    if clf_name == 'min_dist':\n",
    "        y_pred = min_distance_classifier(X_train, y_train, X_test)\n",
    "    \n",
    "    elif clf_name == 'BayesClassifier':\n",
    "        model = BayesClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    elif clf_name == 'naive_bayes':\n",
    "        model = GaussianNB()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    elif clf_name == 'knn':\n",
    "        model = KNeighborsClassifier(n_neighbors=10)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    elif clf_name == 'lda':\n",
    "        model = LinearDiscriminantAnalysis()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    elif clf_name in ['kda_rbf', 'kda_poly', 'kda_linear']:\n",
    "        kernel_type = clf_name.split('_')[1]  # Extracts 'rbf', 'poly', or 'linear'\n",
    "        model = KernelDiscriminantAnalysis(kernel=kernel_type)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown classifier name: {clf_name}\")\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating min_dist...\n",
      "Evaluating BayesClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamra\\AppData\\Local\\Temp\\ipykernel_29068\\4229657362.py:34: RuntimeWarning: divide by zero encountered in log\n",
      "  likelihood = np.sum(np.log(self._calculate_likelihood(self.mean[idx, :], self.variance[idx, :], X)), axis=1)\n",
      "C:\\Users\\iamra\\AppData\\Local\\Temp\\ipykernel_29068\\4229657362.py:34: RuntimeWarning: divide by zero encountered in log\n",
      "  likelihood = np.sum(np.log(self._calculate_likelihood(self.mean[idx, :], self.variance[idx, :], X)), axis=1)\n",
      "C:\\Users\\iamra\\AppData\\Local\\Temp\\ipykernel_29068\\4229657362.py:34: RuntimeWarning: divide by zero encountered in log\n",
      "  likelihood = np.sum(np.log(self._calculate_likelihood(self.mean[idx, :], self.variance[idx, :], X)), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating naive_bayes...\n",
      "Evaluating knn...\n",
      "Evaluating lda...\n",
      "Evaluating kda_rbf...\n",
      "Evaluating kda_poly...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamra\\AppData\\Local\\Temp\\ipykernel_29068\\626085386.py:64: LinAlgWarning: Ill-conditioned matrix (rcond=1.39621e-20): result may not be accurate.\n",
      "  eigvals, eigvecs = np.linalg.eig(solve(N, M))  # Solve for eigenvectors\n",
      "C:\\Users\\iamra\\AppData\\Local\\Temp\\ipykernel_29068\\626085386.py:64: LinAlgWarning: Ill-conditioned matrix (rcond=3.49903e-28): result may not be accurate.\n",
      "  eigvals, eigvecs = np.linalg.eig(solve(N, M))  # Solve for eigenvectors\n",
      "C:\\Users\\iamra\\AppData\\Local\\Temp\\ipykernel_29068\\626085386.py:64: LinAlgWarning: Ill-conditioned matrix (rcond=2.84689e-35): result may not be accurate.\n",
      "  eigvals, eigvecs = np.linalg.eig(solve(N, M))  # Solve for eigenvectors\n",
      "C:\\Users\\iamra\\AppData\\Local\\Temp\\ipykernel_29068\\626085386.py:64: LinAlgWarning: Ill-conditioned matrix (rcond=1.34142e-28): result may not be accurate.\n",
      "  eigvals, eigvecs = np.linalg.eig(solve(N, M))  # Solve for eigenvectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating kda_linear...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamra\\AppData\\Local\\Temp\\ipykernel_29068\\626085386.py:64: LinAlgWarning: Ill-conditioned matrix (rcond=8.15259e-23): result may not be accurate.\n",
      "  eigvals, eigvecs = np.linalg.eig(solve(N, M))  # Solve for eigenvectors\n",
      "C:\\Users\\iamra\\AppData\\Local\\Temp\\ipykernel_29068\\626085386.py:64: LinAlgWarning: Ill-conditioned matrix (rcond=3.77126e-23): result may not be accurate.\n",
      "  eigvals, eigvecs = np.linalg.eig(solve(N, M))  # Solve for eigenvectors\n",
      "C:\\Users\\iamra\\AppData\\Local\\Temp\\ipykernel_29068\\626085386.py:64: LinAlgWarning: Ill-conditioned matrix (rcond=4.66133e-24): result may not be accurate.\n",
      "  eigvals, eigvecs = np.linalg.eig(solve(N, M))  # Solve for eigenvectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL CLASSIFICATION RESULTS\n",
      "================================================================================\n",
      "     Classifier DataVariant  Accuracy  Precision   Recall\n",
      "       min_dist       whole  0.086758   0.244732 0.206489\n",
      "       min_dist         pca  0.091324   0.252673 0.207130\n",
      "       min_dist    kpca_rbf  0.515982   0.103196 0.200000\n",
      "       min_dist   kpca_poly  0.022831   0.102885 0.203540\n",
      "       min_dist    kpca_lin  0.100457   0.182473 0.158289\n",
      "       min_dist       top10  0.063927   0.179937 0.281313\n",
      "BayesClassifier       whole  0.196347   0.182181 0.136878\n",
      "BayesClassifier         pca  0.493151   0.195478 0.196794\n",
      "BayesClassifier    kpca_rbf  0.027397   0.005479 0.200000\n",
      "BayesClassifier   kpca_poly  0.054795   0.327175 0.259637\n",
      "BayesClassifier    kpca_lin  0.178082   0.197662 0.223276\n",
      "BayesClassifier       top10  0.068493   0.170366 0.237898\n",
      "    naive_bayes       whole  0.073059   0.172586 0.244988\n",
      "    naive_bayes         pca  0.493151   0.195478 0.196794\n",
      "    naive_bayes    kpca_rbf  0.315068   0.063014 0.200000\n",
      "    naive_bayes   kpca_poly  0.054795   0.345482 0.233447\n",
      "    naive_bayes    kpca_lin  0.178082   0.197662 0.223276\n",
      "    naive_bayes       top10  0.073059   0.223647 0.245041\n",
      "            knn       whole  0.506849   0.233016 0.223007\n",
      "            knn         pca  0.493151   0.219646 0.208938\n",
      "            knn    kpca_rbf  0.515982   0.103196 0.200000\n",
      "            knn   kpca_poly  0.515982   0.186061 0.209029\n",
      "            knn    kpca_lin  0.465753   0.159397 0.188431\n",
      "            knn       top10  0.502283   0.177392 0.198076\n",
      "            lda       whole  0.356164   0.188578 0.177015\n",
      "            lda         pca  0.520548   0.304147 0.202899\n",
      "            lda    kpca_rbf  0.515982   0.103196 0.200000\n",
      "            lda   kpca_poly  0.515982   0.103196 0.200000\n",
      "            lda    kpca_lin  0.515982   0.103196 0.200000\n",
      "            lda       top10  0.511416   0.102752 0.198230\n",
      "        kda_rbf       whole  0.515982   0.103196 0.200000\n",
      "        kda_rbf         pca  0.515982   0.103196 0.200000\n",
      "        kda_rbf    kpca_rbf  0.013699   0.002740 0.200000\n",
      "        kda_rbf   kpca_poly  0.452055   0.185864 0.200369\n",
      "        kda_rbf    kpca_lin  0.410959   0.139762 0.169180\n",
      "        kda_rbf       top10  0.059361   0.132815 0.293405\n",
      "       kda_poly       whole  0.045662   0.125185 0.282505\n",
      "       kda_poly         pca  0.333333   0.203993 0.214719\n",
      "       kda_poly    kpca_rbf  0.515982   0.103196 0.200000\n",
      "       kda_poly   kpca_poly  0.136986   0.145946 0.182048\n",
      "       kda_poly    kpca_lin  0.328767   0.218688 0.232638\n",
      "       kda_poly       top10  0.109589   0.240434 0.323302\n",
      "     kda_linear       whole  0.045662   0.117290 0.191418\n",
      "     kda_linear         pca  0.073059   0.196802 0.141655\n",
      "     kda_linear    kpca_rbf  0.515982   0.103196 0.200000\n",
      "     kda_linear   kpca_poly  0.077626   0.249225 0.328902\n",
      "     kda_linear    kpca_lin  0.105023   0.223578 0.162316\n",
      "     kda_linear       top10  0.063927   0.179937 0.281313\n",
      "\n",
      "================================================================================\n",
      "TOP PERFORMING CLASSIFIER-DATA VARIANT COMBINATIONS\n",
      "================================================================================\n",
      "Classifier DataVariant  Accuracy  Precision   Recall\n",
      "       lda         pca  0.520548   0.304147 0.202899\n",
      "  min_dist    kpca_rbf  0.515982   0.103196 0.200000\n",
      "       knn    kpca_rbf  0.515982   0.103196 0.200000\n",
      "       knn   kpca_poly  0.515982   0.186061 0.209029\n",
      "       lda    kpca_rbf  0.515982   0.103196 0.200000\n",
      "       lda   kpca_poly  0.515982   0.103196 0.200000\n",
      "       lda    kpca_lin  0.515982   0.103196 0.200000\n",
      "   kda_rbf       whole  0.515982   0.103196 0.200000\n",
      "   kda_rbf         pca  0.515982   0.103196 0.200000\n",
      "  kda_poly    kpca_rbf  0.515982   0.103196 0.200000\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for clf_name in classifiers:\n",
    "    print(f\"Evaluating {clf_name}...\")\n",
    "    for variant_name, (Xtr, Xts) in data_variants.items():\n",
    "        try:\n",
    "            y_pred = run_classifier(clf_name, Xtr, y_train, Xts)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            \n",
    "            results.append((clf_name, variant_name, acc, prec, rec))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error with {variant_name}: {str(e)}\")\n",
    "            results.append((clf_name, variant_name, 0.0, 0.0, 0.0))\n",
    "\n",
    "# Create and display results DataFrame\n",
    "df_results = pd.DataFrame(results, columns=['Classifier', 'DataVariant', 'Accuracy', 'Precision', 'Recall'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL CLASSIFICATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Display best performing combinations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP PERFORMING CLASSIFIER-DATA VARIANT COMBINATIONS\")\n",
    "print(\"=\"*80)\n",
    "top_results = df_results.nlargest(10, 'Accuracy')\n",
    "print(top_results.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
